# 数字花园 (Digital Garden) - 技术架构与学习复盘

> 日期：2025-11-20
> 项目阶段：MVP 核心功能完成

## 一、 核心理论：Transformer 架构与 Embedding

本项目利用 AI 技术实现了从“文本”到“语义向量”的转化，其底层基于 Transformer 架构。

### 1. Transformer 架构理解

Transformer 就像一个“高维语义提取机”。

- **核心机制**：**自注意力机制 (Self-Attention)**。它改变了传统 RNN 必须顺序阅读的限制，能够“上帝视角”并行计算句子中所有词之间的关联强度（例如理解“它”指代的是“苹果”还是“红富士”）。
- **模型选择**：本项目使用了 **Encoder-only** 架构的模型（如 BERT 系列的 `all-MiniLM-L6-v2`），专注于理解语义和提取特征，而非生成文本（Decoder 架构如 GPT 所擅长的）。

### 2. 向量化 (Embedding)

- **原理**：将自然语言映射到高维数学空间（本项目为 384 维）。
- **意义**：计算机从此不再比较“字面是否相同”，而是计算“向量距离 (Cosine Similarity)”。这是实现“灵感碰撞”、“语义搜索”的数学基石。

## 二、 工程实践：全栈 AI 开发 (Full Stack AI)

### 1. 向量数据库 (Vector Database)

- **选型**：Supabase (`pgvector` 扩展)。
- **应用**：在 Postgres 关系型数据库中实现了向量存储，支持对非结构化文本进行语义查询。

### 2. 数据可视化与降维 (Dimensionality Reduction)

- **难点**：384 维的向量空间人类无法理解。
- **解决方案**：引入 **UMAP 算法**。
- **效果**：将高维数据映射到 2D 平面，同时保持了数据内部的拓扑结构（语义相近的点在平面上也靠在一起），形成了“星空图谱”。

### 3. 前端性能优化 (Performance Engineering)

为了解决大规模向量计算导致的浏览器卡顿，采用了以下工程手段：

- **Web Worker**：将 UMAP 计算任务从主线程剥离到后台线程，避免阻塞 UI 渲染。
- **乐观更新 (Optimistic UI)**：在网络请求返回前先行更新界面，实现“零延迟”的用户输入体验。

---

_By: Digital Garden Team_
